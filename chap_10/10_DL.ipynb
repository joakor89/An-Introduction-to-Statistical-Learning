{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Computing\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib .pyplot import subplots\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn. pipeline import Pipeline\n",
    "from sklearn. model_selection import KFold\n",
    "from sklearn. preprocessing import StandardScaler\n",
    "from sklearn. model_selection import (train_test_split, GridSearchCV)\n",
    "from sklearn. linear_model import (LinearRegression, LogisticRegression, Lasso)\n",
    "\n",
    "# ISLP\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "\n",
    "# JavaScript Object Notation\n",
    "import json\n",
    "\n",
    "# Glob\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#### Torch-Specific Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import RMSprop\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from torchvision.io import read_image\n",
    "from torchmetrics import (MeanAbsoluteError, R2Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.utilities.seed import seed_everything\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(0, workers=True)\n",
    "\n",
    "torch.use_deterministic_algorithms(True, warn_only =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR100\n",
    "from torchvision.models import (resnet50, ResNet50_Weights)\n",
    "from torchvision.transforms import (Resize, Normalize, CenterCrop,ToTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.torch import (SimpleDataModule, SimpleModule, ErrorTracker, rec_num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.torch.imdb import (load_lookup, load_tensor, load_sparse, load_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Single Layer Network on Hitters Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = load_data('Hitters').dropna()\n",
    "\n",
    "n = Hitters.shape [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MS(Hitters.columns.drop('Salary'), intercept =False)\n",
    "\n",
    "X = model.fit_transform(Hitters).to_numpy()\n",
    "Y = Hitters['Salary']. to_numpy ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,\n",
    " X_test,\n",
    " Y_train,\n",
    " Y_test) = train_test_split(X,\n",
    "                            Y,\n",
    "                            test_size=1/3,\n",
    "                            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_lm = LinearRegression().fit(X_train, Y_train)\n",
    "Yhat_test = hit_lm.predict(X_test)\n",
    "np.abs( Yhat_test - Y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "lasso = Lasso(warm_start=True , max_iter=30000)\n",
    "\n",
    "standard_lasso = Pipeline(steps=[('scaler', scaler),\n",
    "                                  ('lasso', lasso)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = scaler.fit_transform(X_train)\n",
    "n = X_s.shape[0]\n",
    "\n",
    "lam_max = np.fabs(X_s.T.dot(Y_train - Y_train.mean ())).max() / n\n",
    "param_grid = {'alpha': np.exp(np.linspace(0, np.log(0.01), 100))\n",
    "              * lam_max}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(10,\n",
    "           shuffle=True,\n",
    "           random_state=1)\n",
    "\n",
    "grid = GridSearchCV(lasso,\n",
    "                     param_grid,\n",
    "                    cv=cv,\n",
    "                    scoring='neg_mean_absolute_error')\n",
    "\n",
    "grid.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_lasso = grid.best_estimator_\n",
    "\n",
    "Yhat_test = trained_lasso.predict(X_test)\n",
    "\n",
    "np.fabs(Yhat_test - Y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Specifying a Network: `Classes and Inheritance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HittersModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HittersModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self. sequential = nn.Sequential(\n",
    "            nn.Linear(input_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear (50, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return torch.flatten(self.sequential(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_model = HittersModel(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(hit_model,\n",
    "        input_size=X_train.shape,\n",
    "        col_names=['input_size',\n",
    "                   'output_size',\n",
    "                   'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train.astype(np.float32))\n",
    "Y_train_t = torch.tensor(Y_train.astype(np.float32))\n",
    "\n",
    "hit_train = TensorDataset(X_train_t , Y_train_t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = torch.tensor(X_test.astype(np.float32))\n",
    "Y_test_t = torch.tensor(Y_test.astype(np.float32))\n",
    "\n",
    "hit_test = TensorDataset(X_test_t, Y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_workers = rec_num_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_dm = SimpleDataModule(hit_train,\n",
    "                          hit_test,\n",
    "                          batch_size=32,\n",
    "                          num_workers =min(4, max_num_workers),\n",
    "                          validation = hit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_module = SimpleModule.regression(hit_model,\n",
    "                                     metrics={'mae': MeanAbsoluteError()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_logger = CSVLogger('logs', name='hitters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_trainer = Trainer(deterministic=True,\n",
    "                      max_epochs=50,\n",
    "                      log_every_n_steps=5,\n",
    "                      logger=hit_logger,\n",
    "                      callbacks=[ErrorTracker()])\n",
    "\n",
    "hit_trainer .fit(hit_module , datamodule =hit_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_trainer.test(hit_module, datamodule=hit_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_results = pd.read_csv(hit_logger.experiment. metrics_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_plot(results,\n",
    "                 ax,\n",
    "                 col='loss',\n",
    "                 valid_legend ='Validation',\n",
    "                 training_legend ='Training',\n",
    "                 ylabel='Loss',\n",
    "                 fontsize =20):\n",
    "    for (column,\n",
    "         color,\n",
    "         label) in zip ([f'train_{col}_epoch',\n",
    "                         f'valid_{col}'],\n",
    "                        ['black', 'red'],\n",
    "                        [training_legend, valid_legend]):\n",
    "        results.plot(x='epoch',\n",
    "                     y=column,\n",
    "                     label=label,\n",
    "                     marker='o',\n",
    "                     color=color,\n",
    "                     ax=ax)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax = summary_plot(hit_results,\n",
    "                  ax,\n",
    "                  col='mae',\n",
    "                  ylabel='MAE',\n",
    "                  valid_legend='Validation(=Test)')\n",
    "\n",
    "ax.set_ylim([0, 400])\n",
    "ax.set_xticks(np.linspace(0, 50, 11).astype(int));\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_model.eval()\n",
    "preds = hit_module(X_test_t)\n",
    "torch.abs(Y_test_t - preds).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(Hitters,\n",
    "    hit_model, \n",
    "    hit_dm,\n",
    "    hit_logger,\n",
    "    hit_test, hit_train,\n",
    "    X, Y,\n",
    "    X_test, X_train,\n",
    "    Y_test, Y_train,\n",
    "    X_test_t, Y_test_t,\n",
    "    hit_trainer, hit_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Multilayer Network on the MNIST Digit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train,\n",
    " mnist_test) = [MNIST(root='data',\n",
    "                      train=train,\n",
    "                      download=True,\n",
    "                      transform = ToTensor())\n",
    "                for train in [True, False]]\n",
    "\n",
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dm = SimpleDataModule(mnist_train,\n",
    "                            mnist_test,\n",
    "                            validation=0.2,\n",
    "                            num_workers=max_num_workers,\n",
    "                            batch_size =256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (X_, Y_) in enumerate(mnist_dm.train_dataloader()):\n",
    "    print('X:', X_.shape)\n",
    "    print('Y:', Y_.shape)\n",
    "    \n",
    "    if idx >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3))\n",
    "        self._forward = nn.Sequential(\n",
    "            self.layer1,\n",
    "            self.layer2,\n",
    "            nn.Linear(128, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model(X_).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(mnist_model,\n",
    "        input_data=X_,\n",
    "        col_names =['input_size',\n",
    "                    'output_size',\n",
    "                    'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_module = SimpleModule.classification(mnist_model, num_classes=10)\n",
    "\n",
    "mnist_logger = CSVLogger('logs', name='MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainer = Trainer(deterministic=True,\n",
    "                        max_epochs=30,\n",
    "                        logger=mnist_logger,\n",
    "                        callbacks =[ErrorTracker()])\n",
    "\n",
    "mnist_trainer.fit(mnist_module,\n",
    "                  datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_results = pd.read_csv(mnist_logger.experiment.metrics_file_path)\n",
    "\n",
    "fig, ax = subplots(1, 1, figsize=(6, 6))\n",
    "summary_plot(mnist_results,\n",
    "             ax,\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "\n",
    "ax.set_ylim([0.5, 1])\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(np.linspace(0, 30, 7).astype(int));\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainer.test(mnist_module,\n",
    "                   datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_MLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_MLR, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(784, 10))\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "mlr_model = MNIST_MLR()\n",
    "mlr_module = SimpleModule.classification(mlr_model, num_classes=10)\n",
    "mlr_logger = CSVLogger('logs', name='MNIST_MLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_trainer = Trainer(deterministic=True,\n",
    "                      max_epochs=30,\n",
    "                      callbacks=[ErrorTracker()])\n",
    "\n",
    "mlr_trainer.fit(mlr_module, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_trainer .test(mlr_module,\n",
    "                  datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(mnist_test,\n",
    "    mnist_train,\n",
    "    mnist_model,\n",
    "    mnist_dm,\n",
    "    mnist_trainer,\n",
    "    mnist_module,\n",
    "    mnist_results,\n",
    "    mlr_model,\n",
    "    mlr_module,\n",
    "    mlr_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "(cifar_train,\n",
    " cifar_test ) = [ CIFAR100 (root=\"data\",\n",
    "                                        train=train,\n",
    "                                        download=True)\n",
    "                              for train in [True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "\n",
    "cifar_train_X = torch.stack([transform(x) for x in\n",
    "    cifar_train.data])\n",
    "\n",
    "cifar_test_X = torch.stack([transform(x) for x in\n",
    "    cifar_test.data])\n",
    "\n",
    "cifar_train = TensorDataset(cifar_train_X,\n",
    "                            torch.tensor(cifar_train.targets))\n",
    "\n",
    "cifar_test = TensorDataset(cifar_test_X,\n",
    "                           torch.tensor(cifar_test.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_dm = SimpleDataModule (cifar_train,\n",
    "                             cifar_test,\n",
    "                             validation=0.2,\n",
    "                             num_workers =max_num_workers,\n",
    "                             batch_size =128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (X_, Y_) in enumerate(cifar_dm.train_dataloader()):\n",
    "    print('X: ', X_.shape)\n",
    "    print('Y: ', Y_.shape)\n",
    "    if idx >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "indices = rng.choice(np.arange(len(cifar_train)), 25,\n",
    "                     replace=False).reshape((5, 5))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        idx = indices[i,j]\n",
    "        axes[i,j]. imshow(np. transpose(cifar_train[idx][0],\n",
    "                                        [1, 2, 0]),\n",
    "                          interpolation =None)\n",
    "        axes[i,j].set_xticks([])\n",
    "        axes[i,j].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels):\n",
    "        super(BuildingBlock, self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels=in_channels,\n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=(3, 3),\n",
    "                            padding='same')\n",
    "        self.activation=nn.ReLU()\n",
    "        self.pool=nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        def forward(self , x):\n",
    "            return self.pool(self. activation (self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFARModel, self).__init__()\n",
    "        sizes=[(3, 32),\n",
    "               (32, 64),\n",
    "               (64, 128),\n",
    "               (128, 256)]\n",
    "        self.conv=nn.Sequential(*[ BuildingBlock(in_, out_)\n",
    "                                   for in_, out_ in sizes])\n",
    "        self.output=nn.Sequential(nn.Dropout(0.5),\n",
    "                                  nn.Linear (2*2*256, 512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(512, 100))\n",
    "    def forward(self, x):\n",
    "        val = self.conv(x)\n",
    "        val = torch.flatten(val, start_dim =1)\n",
    "        return self.output(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_model = CIFARModel()\n",
    "\n",
    "summary(cifar_model,\n",
    "        input_data=X_,\n",
    "        col_names =['input_size',\n",
    "                    'output_size',\n",
    "                    'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_optimizer = RMSprop(cifar_model.parameters(), lr =0.001)\n",
    "\n",
    "cifar_module = SimpleModule.classification(cifar_model,\n",
    "                                           optimizer=cifar_optimizer)\n",
    "\n",
    "cifar_logger = CSVLogger('logs', name='CIFAR100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_trainer = Trainer(deterministic=True,\n",
    "                        max_epochs=30,\n",
    "                        logger=cifar_logger,\n",
    "                        callbacks=[ErrorTracker()])\n",
    "\n",
    "cifar_trainer.fit(cifar_module,\n",
    "                  datamodule=cifar_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = cifar_logger.experiment.metrics_file_path\n",
    "\n",
    "cifar_results = pd.read_csv(log_path)\n",
    "\n",
    "fig, ax =subplots(1, 1, figsize=(6, 6))\n",
    "summary_plot(cifar_results,\n",
    "             ax,\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "ax.set_xticks(np.linspace(0, 10, 6).astype(int))\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_trainer.test(cifar_module,\n",
    "                   datamodule=cifar_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Hardware Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for name, metric in cifar_module.metrics.items():\n",
    "        cifar_module.metrics[name]=metric.to('mps')\n",
    "    cifar_trainer_mps=Trainer(accelerator='mps',\n",
    "                              deterministic=True,\n",
    "                              max_epochs=30)\n",
    "    cifar_trainer_mps.fit(cifar_module,\n",
    "                          datamodule=cifar_dm)\n",
    "    cifar_trainer_mps.test(cifar_module,\n",
    "                           datamodule=cifar_dm)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Using Pretrained CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = Resize((232, 232))\n",
    "crop = CenterCrop(224)\n",
    "\n",
    "normalize = Normalize([0.485, 0.456, 0.406],\n",
    "                      [0.229, 0.224, 0.225])\n",
    "\n",
    "imgfiles = sorted([f for f in glob('book_images /*')])\n",
    "imgs = torch.stack([torch.div(crop(resize(read_image(f))), 255)\n",
    "                    for f in imgfiles ])\n",
    "\n",
    "imgs = normalize(imgs)\n",
    "imgs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "summary(resnet_model,\n",
    "        input_data=imgs,\n",
    "        col_names =['input_size',\n",
    "                    'output_size',\n",
    "                    'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_preds = resnet_model(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_probs = np.exp(np.asarray(img_preds.detach()))\n",
    "\n",
    "img_probs /= img_probs.sum(1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = json.load(open('imagenet_class_index .json'))\n",
    "\n",
    "class_labels = pd.DataFrame([(int(k), v[1]) for k, v in\n",
    "                            labs.items()],\n",
    "                            columns =['idx', 'label'])\n",
    "\n",
    "class_labels = class_labels.set_index('idx')\n",
    "class_labels = class_labels.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, imgfile in enumerate(imgfiles):\n",
    "    img_df = class_labels.copy()\n",
    "    img_df['prob'] = img_probs[i]\n",
    "    img_df = img_df.sort_values(by='prob', ascending=False)[:3]\n",
    "    print(f'Image: {imgfile}')\n",
    "    print(img_df.reset_index().drop(columns=['idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(cifar_test,\n",
    "    cifar_train,\n",
    "    cifar_dm,\n",
    "    cifar_module,\n",
    "    cifar_logger,\n",
    "    cifar_optimizer,\n",
    "    cifar_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### IMDB Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "(imdb_seq_train,\n",
    " imdb_seq_test) = load_sequential(root='data/IMDB')\n",
    "\n",
    "padded_sample = np.asarray(imdb_seq_train.tensors[0][0])\n",
    "sample_review = padded_sample[padded_sample > 0][:12]\n",
    "\n",
    "sample_review[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = load_lookup(root='data/IMDB')\n",
    "\n",
    "' '.join(lookup[i] for i in sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_workers=10\n",
    "\n",
    "(imdb_train,\n",
    " imdb_test) = load_tensor(root='data/IMDB')\n",
    "\n",
    "imdb_dm = SimpleDataModule(imdb_train,\n",
    "                           imdb_test,\n",
    "                           validation=2000,\n",
    "                           num_workers=min(6, max_num_workers),\n",
    "                           batch_size =512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBModel (nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super(IMDBModel, self).__init__()\n",
    "        self.dense1 = nn.Linear(input_size , 16)\n",
    "        self. activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(16, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self , x):\n",
    "        val = x\n",
    "        for _map in[self.dense1,\n",
    "                    self.activation,\n",
    "                    self.dense2,\n",
    "                    self.activation,\n",
    "                    self.output]:\n",
    "            val = _map(val)\n",
    "        return torch.flatten(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model = IMDBModel(imdb_test.tensors[0].size()[1])\n",
    "summary(imdb_model,\n",
    "        input_size = imdb_test.tensors[0].size(),\n",
    "        col_names =['input_size',\n",
    "                    'output_size',\n",
    "                    'num_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_optimizer = RMSprop(imdb_model.parameters(), lr =0.001)\n",
    "\n",
    "imdb_module = SimpleModule.binary_classification(\n",
    "    imdb_model,\n",
    "    optimizer=imdb_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_logger = CSVLogger('logs', name='IMDB')\n",
    "\n",
    "imdb_trainer = Trainer(deterministic=True,\n",
    "                       max_epochs=30,\n",
    "                       logger=imdb_logger,\n",
    "                       callbacks=[ErrorTracker()])\n",
    "\n",
    "imdb_trainer.fit(imdb_module,\n",
    "                 datamodule =imdb_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = imdb_trainer.test(imdb_module, datamodule=imdb_dm)\n",
    "\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### Comparison to Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "((X_train, Y_train),\n",
    " (X_valid, Y_valid),\n",
    " (X_test, Y_test)) = load_sparse(validation=2000,\n",
    "                                 random_state =0,\n",
    "                                 root='data/IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_max = np.abs(X_train.T * (Y_train - Y_train.mean())).max()\n",
    "\n",
    "lam_val = lam_max * np.exp(np.linspace(np.log(1),\n",
    "                                       np.log(1e -4), 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression (penalty='l1',\n",
    "                            C=1/ lam_max,\n",
    "                            solver='liblinear',\n",
    "                            warm_start =True,\n",
    "                            fit_intercept =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "\n",
    "intercepts = []\n",
    "\n",
    "for l in lam_val:\n",
    "    logit.C = 1/l\n",
    "        logit.fit(X_train, Y_train)\n",
    "\n",
    "coefs.append(logit.coef_.copy())\n",
    "\n",
    "intercepts.append(logit.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.squeeze(coefs)\n",
    "\n",
    "intercepts = np.squeeze(intercepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% capture\n",
    "fig , axes = subplots(1, 2, figsize=(16, 8), sharey=True)\n",
    "\n",
    "for ((X_, Y_),\n",
    "     data_,\n",
    "     color) in zip ([(X_train , Y_train),\n",
    "                     (X_valid , Y_valid),\n",
    "                     (X_test , Y_test)],\n",
    "                    ['Training', 'Validation', 'Test'],\n",
    "                    ['black', 'red', 'blue']):\n",
    "    linpred_ = X_ * coefs.T + intercepts[None, :]\n",
    "    label_ = np.array(linpred_ > 0)\n",
    "    accuracy_ = np.array([np.mean(Y_ == l) for l in label_.T])\n",
    "    axes [0]. plot(-np.log(lam_val / X_train.shape[0]),\n",
    "                   accuracy_,\n",
    "                   '.--',\n",
    "                   color=color,\n",
    "                   markersize=13,\n",
    "                   linewidth=2,\n",
    "                   label=data_)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlabel(r'$-\\ log (\\ lambda)$', fontsize=20)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_results = pd.read_csv(imdb_logger.experiment.metrics_file_path)\n",
    "\n",
    "summary_plot(imdb_results,\n",
    "             axes[1],\n",
    "             col='accuracy',\n",
    "             ylabel='Accuracy')\n",
    "\n",
    "axes[1].set_xticks(np.linspace(0, 30, 7).astype(int))\n",
    "axes[1].set_ylabel('Accuracy ', fontsize=20)\n",
    "axes[1].set_xlabel('Epoch ', fontsize=20)\n",
    "axes[1].set_ylim([0.5 , 1]);\n",
    "axes[1].axhline(test_results[0]['test_accuracy'],\n",
    "                color='blue',\n",
    "                linestyle='--',\n",
    "                linewidth=3)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(imdb_model,\n",
    "    imdb_trainer,\n",
    "    imdb_logger,\n",
    "    imdb_dm,\n",
    "    imdb_train,\n",
    "    imdb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ISLP] *",
   "language": "python",
   "name": "conda-env-ISLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
